## パーセプトロン
パーセプトロンとは複数信号を受け取り、一つの信号を出力するアルゴリズムです。このアルゴリズムはニューラルネットワークを学ぶ上で、非常に重要な考え方となります。

<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/48417135-552dac00-e795-11e8-897f-3057dcea5d45.png" alt="Fig. 2.1" height="200px">
</div>
<div align="center">
Fig. 2.1
</div>
　

Fig. 2.1はパーセプトロンを図示したもので、x は入力、w は重み、y は出力を表しています。
※Fig. 1.2の表現と若干違うのは本書籍における表現方法と合わせたからである。根本的な考え方に相違はありません。

入力x それぞれに重みw を乗算し、その総和がある値(閾値)を超えた場合にのみ出力y が「1」となり、それ以外の場合は「0」となります。これは以下のような数式で表すことができる。

<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49093955-900d0500-f2a8-11e8-88ea-0062295da7c4.png" alt="数式2.1" height="50px">
</div>
　

ここで、θ は閾値を表し、1 が出力されることを「**発火**」と呼びます。各パラメータはそれぞれ機能を持っていて、重みw は各入力信号の**重要度をコントロール**し、閾値θ は**発火のしやすさを調整する**機能があります。数式(2.1)をみれば閾値θ が大きければ発火しづらく、小さければ発火しやすいことは容易に想像できると思います。

このパーセプトロンを拡張することでニューラルネットワークを表現することができます。本書籍2.2以降の節ではAND ゲートOR ゲートといったような論理回路をパーセプトロンで表現することで、パーセプトロンの限界と多層化による表現の拡張について説明しています。

ここでは、パーセプトロンについてこれ以上掘り下げませんので、単層のパーセプトロンでは表現に限界があるものの、**層を重ねることによってより複雑な表現も可能である**ということだけ覚えておいてください。

## パーセプトロンからニューラルネットワークへ
### バイアスの導入

今後の説明をスムーズに進めるために、数式(2.1)の閾値θ を-b で置換して、以下のように修正します。

<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49093325-2e986680-f2a7-11e8-91e7-b52e0299fcd4.png" alt="数式2.2" height="50px">
</div>
　

そして、この新しいパラメータb を「**バイアス**」 と呼びます。バイアスb も閾値θ と同様に発火のしやすさを調整する機能を持っています。

バイアスb を考慮してパーセプトロンを図示すると、

<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49328532-df5b7a00-f5b5-11e8-87ec-9095235e4f82.png" alt="Fig. 2.2" height="200px">
</div>
<div align="center">
Fig. 2.2
</div>
　
 
のようになります。バイアスは入力信号が1 で重みがb の信号と考えることができるわけです。

### 活性化関数の登場
ここで、数式(2.2)を以下のように書き換えます。
 
<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49325184-807c0d80-f581-11e8-9f63-832046a2e1dc.png" alt="数式2.3-4" height="80px">
</div>
　

h(x) という関数を用いることで、非常にシンプルな形で表現することができます。
さらに、数式(2.3)を、

<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49325363-74457f80-f584-11e8-9641-f7af240c07ab.png" alt="数式2.5-6" height="60px">
</div>
　
 
 と書き換えることで、以下のようにパーセプトロンを図示することができます。
 
 <div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49329642-73810d80-f5c5-11e8-866a-d27d291a2747.png" alt="Fig. 2.3" height="200px">
</div>
<div align="center">
Fig. 2.3
</div>
　
 
重み付きの入力信号の総和の結果がa となり、関数h(x) によって出力y に変換されるというプロセスを明示するような形で出力のニューロンを表現しています。  
h(x) のように入力信号の総和を出力信号に変換するような関数を「**活性化関数**」と呼びます。この活性化関数によって出力値を決定することができるわけです。

ここまで、パーセプトロンで使われてきた活性化関数は、以下のような「**ステップ関数**」と呼ばれる関数でした(数式(2.4)を図示したものです)。

<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49341222-c32b0c00-f68d-11e8-9fb3-1c4540552727.png" alt="Fig. 2.4" height="200px">
</div>
<div align="center">
Fig. 2.4
</div>
　

パーセプトロンからニューラルネットワークに拡張するためには、**活性化関数をステップ関数以外の関数に変える**必要があります。

※なぜ、ステップ関数はニューラルネットワークで使えないのかというと、ステップ関数のようにほとんどの領域(0 以外の領域)で微分が0 となるような関数の場合、ニューラルネットワークの特徴である「学習」がうまく行えないからなのですが、こちらについての詳細は後述します。

### 活性化関数の種類
ではここで、ニューラルネットワークで活性化関数として用いられている関数を2つほど紹介します。

#### シグモイド関数
まず紹介するのは、「**シグモイド関数**」と呼ばれる関数で、以下の式で定義されます。

<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49382201-23d34b00-f759-11e8-90a6-fc8d7bed48bd.png" alt="数式2.7" height="50px">
</div>
　

そして、数式(2.7)を図示したものが以下のグラフになります。

<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49341356-a8f22d80-f68f-11e8-8067-c2cd7b852c9c.png" alt="Fig. 2.5" height="200px">
</div>
<div align="center">
Fig. 2.5
</div>
　
 
 数式およびグラフを見れば、入力が大きくなるほど出力は1 に近づき、入力が小さくなるほど出力は0 に近づくことがわかると思います。つまり、巨視的に見れば、これまで使ってきたステップ関数と同じような形をしていると考えることができます。
 
 シグモイド関数とステップ関数の最も大きな違いは、連続性にあります。ステップ関数は入力0 を境に0 か1 のどちらかの値を出力します。しかし、シグモイド関数は連続的な実数値を返却します。
 
#### ReLu 関数

<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/49341362-b7d8e000-f68f-11e8-9ec2-d732cc8fe6bc.png" alt="Fig. 2.6" height="200px">
</div>
<div align="center">
Fig. 2.6
</div>

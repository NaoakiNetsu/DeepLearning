## はじめに

書籍『ゼロから作るDeep Learning』（以下、本書籍）の内容をまとめる。

## ディープラーニングとニューラルネットワーク

本書籍でディープラーニングの理論を学ぶ前に、ディープラーニングが生まれるまでの歴史を簡単に振り返る。

### 形式ニューロンの誕生
脳は多数の神経細胞（ニューロン）のつながり（ネットワーク）によって形成される。1943年、この神経細胞をモデル化した「形式ニューロン」が発表された。
<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/48361078-0a079080-e6e4-11e8-9c2a-5dad0a2a831b.png" alt="Fig. 1.1"></br>
Fig. 1.1
</div>
</br>
ここで、x は別ニューロンからの入力、w は重み、θ は閾値を表す。別ニューロンからの各入力x に対してそれぞれ重みw を掛け合わせた値の総和がある値θ を超えた場合、次のニューロンに伝える出力は1 、越えなかった場合、出力は0 となる。例えば、入力が(1,3,5) で重みが(0.5,0.9,0.1) である場合、総和は1×0.5+3×0.9+5×0.1 = 3.7 。この場合、閾値が3 であれば1 が出力され、閾値が4 であれば0 が出力されることになる。 これによって1つの神経細胞を非常に簡略化して表現している。

### パーセプトロンへの拡張
形式ニューロンモデルの考え方に基づいて、1958年に開発されたのが「パーセプトロン」である。
<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/48382026-64bfdd00-e722-11e8-90a8-51095611fa0a.png" alt="Fig. 1.2"></br>
Fig. 1.2
</div>
</br>
開発したとは言っても、Fig. 1.2を見ればわかるように形式ニューロンを入力層と出力層の2層に拡張しただけである。

しかしながら、実はこれがとても重要で、入力層と出力層に分けることよって重みw を最適化することが可能となる。この重みを調整する作業を「学習」と呼ぶ。つまり、パーセプトロンにより簡単な「学習」ができるようになったというわけである。

とはいえ、この「学習」には限度があり、理論的に解けない問題が多く存在することが発覚し、次第に日の目を浴びなっていった。

### ニューラルネットワークの誕生と衰退
1958年に発表されたパーセプトロンでは入力層と出力層の間の重みを調整することで学習できるようになったが、理論的に解けない問題、具体的には非線形分離問題を解けないという難点があった。

この課題を解決する方法は単純で、入力層と出力層の間に中間層を挿入するだけである。
<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/48417098-41824580-e795-11e8-818b-b60549a545f7.png" alt="Fig. 1.3"></br>
Fig. 1.3
</div>
</br>
このように層を増やしていき、更新する重みの数を増やすことで入力・出力層だけの単純なパーセプトロンで解決できない問題も解くことができるようになった。以降は、入力層と出力層だけのパーセプトロンを「単純パーセプトロン」、単純パーセプトロンに中間層を挿入したものを「多層パーセプトロン」と呼ぶことにする。  
<br>
また、多層パーセプトロンの学習を効率的に行うアルゴリズムも考案された。

### ニューラルネットワークの復活
準備中


## パーセプトロン
パーセプトロンとは複数信号を受け取り、一つの信号を出力するアルゴリズムである。このアルゴリズムはニューラルネットワークを学ぶ上で、非常に重要な考え方となる。
<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/48417135-552dac00-e795-11e8-897f-3057dcea5d45.png" alt="Fig. 2.1"></br>
Fig. 2.1
</div>
</br>
Fig. 2.1はパーセプトロンを図示したもので、x は入力、w は重み、y は出力を表している。
※Fig. 1.2の表現と若干違うのは本書籍における表現方法と合わせたからである。根本的な考え方に相違はない。

入力x それぞれに重みw を乗算し、その総和がある値(閾値)を超えた場合にのみ出力y が「1」となり、それ以外の場合は「0」となる。これは以下のような数式で表すことができる。
<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/48417210-8312f080-e795-11e8-83c8-2ea44ec3481e.png" alt="数式2.1"></br>
</div>
</br>
ここで、θ は閾値を表している。1 が出力されることを「発火」と呼ぶ。各パラメータはそれぞれ機能を持っていて、重みw は各入力信号の重要度をコントロールし、閾値θ は「発火」のしやすさを調整する機能がある。数式(2.1)をみれば閾値θ が大きければ発火しづらく、小さければ発火しやすいことは容易に想像できるだろう。
  
このパーセプトロンを拡張することでニューラルネットワークを表現することができる。以下では、ニューラルネットワークを理解するために、もう少しパーセプトロンについて掘り下げていこうと思う。

## はじめに

書籍『ゼロから作るDeep Learning』の内容をまとめる。

## ディープラーニングとニューラルネットワーク

本書籍でディープラーニングの理論を学ぶ前に、ディープラーニングが生まれるまでの歴史を簡単に振り返る。

### 形式ニューロンの誕生
脳は多数の神経細胞（ニューロン）のつながり（ネットワーク）によって形成される。1943年、この神経細胞をモデル化した「形式ニューロン」が発表された。
<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/48361078-0a079080-e6e4-11e8-9c2a-5dad0a2a831b.png" alt="Fig. 1.1"></br>
Fig. 1.1
</div>
</br>
ここで、x は別ニューロンからの入力、w は重み、θ は閾値を表す。別ニューロンからの各入力x に対してそれぞれ重みw を掛け合わせた値の総和がある値θ を超えた場合、次のニューロンに伝える出力は1 、越えなかった場合、出力は0 となる。例えば、入力が(1,3,5) で重みが(0.5,0.9,0.1) である場合、総和は1×0.5+3×0.9+5×0.1 = 3.7 。この場合、閾値が3 であれば1 が出力され、閾値が4 であれば0 が出力されることになる。 これによって1つの神経細胞を非常に簡略化して表現している。

### パーセプトロンへの拡張
形式ニューロンモデルの考え方に基づいて、1958年に開発されたのが「パーセプトロン」である。
<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/48382026-64bfdd00-e722-11e8-90a8-51095611fa0a.png" alt="Fig. 1.2"></br>
Fig. 1.2
</div>
</br>
開発したとは言っても、Fig. 1.2を見ればわかるように形式ニューロンを入力層と出力層の2層に拡張しただけである。

しかしながら、実はこれがとても重要で、入力層と出力層に分けることよって重みw を最適化することが可能となる。この重みを調整する作業を「学習」と呼ぶ。つまり、パーセプトロンにより簡単な「学習」ができるようになったというわけである。

とはいえ、この「学習」には限度があり、理論的に解けない問題が多く存在することが発覚し、次第に日の目を浴びなっていった。

### ニューラルネットワークの誕生と衰退
1958年に発表されたパーセプトロンでは入力層と出力層の間の重みを調整することで学習できるようになったが、理論的に解けない問題、具体的には非線形分離問題を解けないという難点があった。

この課題を解決する方法は単純で、入力層と出力層の間に中間層を挿入するだけである。
<div align="center">
<img src="https://user-images.githubusercontent.com/28583094/48417098-41824580-e795-11e8-818b-b60549a545f7.png" alt="Fig. 1.3"></br>
Fig. 1.3
</div>
</br>
このように層を増やしていき、更新する重みの数を増やすことで入力・出力層だけの単純なパーセプトロンで解決できない問題も解くことができるようになった。以降は、入力層と出力層だけのパーセプトロンを「単純パーセプトロン」、単純パーセプトロンに中間層を挿入したものを「多層パーセプトロン」と呼ぶことにする。
</br>
また、多層パーセプトロンの学習を効率的に行うアルゴリズムも考案された。

### ニューラルネットワークの復活
準備中
